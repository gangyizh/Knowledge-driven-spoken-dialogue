{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddc573a8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "attempted relative import with no known parent package",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1798246/330138565.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     25\u001b[0m )\n\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 27\u001b[0;31m from dataset import (\n\u001b[0m\u001b[1;32m     28\u001b[0m     \u001b[0mResponseGenerationDataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m     \u001b[0mSPECIAL_TOKENS\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/data1/zhousf/hwbase_c/Generation/baseline_ch/dataset.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m )\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscripts\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset_walker\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDatasetWalker\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0mlogger\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlogging\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetLogger\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mImportError\u001b[0m: attempted relative import with no known parent package"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import logging\n",
    "import os\n",
    "import random\n",
    "import json\n",
    "\n",
    "from typing import Dict, Tuple\n",
    "from argparse import Namespace\n",
    "\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import torch\n",
    "\n",
    "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
    "from torch.utils.data.distributed import DistributedSampler\n",
    "from tqdm import tqdm, trange\n",
    "from transformers import (\n",
    "    AdamW,\n",
    "    AutoConfig,\n",
    "    BertTokenizer,\n",
    "    GPT2LMHeadModel,\n",
    "    PreTrainedModel,\n",
    "    PreTrainedTokenizer,\n",
    "    get_linear_schedule_with_warmup,\n",
    ")\n",
    "\n",
    "from dataset import (\n",
    "    ResponseGenerationDataset,\n",
    "    SPECIAL_TOKENS\n",
    ")\n",
    "from utils.argument import (\n",
    "    set_default_params,\n",
    "    set_default_dataset_params,\n",
    "    update_additional_params,\n",
    "    verify_args\n",
    ")\n",
    "from utils.model import (\n",
    "    run_batch_generation\n",
    ")\n",
    "from utils.data import write_selection_preds, write_detection_preds\n",
    "\n",
    "try:\n",
    "    from torch.utils.tensorboard import SummaryWriter\n",
    "except ImportError:\n",
    "    from tensorboardX import SummaryWriter\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "\n",
    "def get_classes(task):\n",
    "    if task.lower() == \"generation\":\n",
    "        return ResponseGenerationDataset, GPT2LMHeadModel, \\\n",
    "               run_batch_generation, run_batch_generation\n",
    "    else:\n",
    "        raise ValueError(\"args.task not in \"\n",
    "                         \"['generation', 'selection', 'detection'], got %s\" % task)\n",
    "\n",
    "\n",
    "def set_seed(args):\n",
    "    random.seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    torch.manual_seed(args.seed)\n",
    "    if args.n_gpu > 0:\n",
    "        torch.cuda.manual_seed_all(args.seed)\n",
    "\n",
    "\n",
    "def train(args, train_dataset, eval_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, run_batch_fn_train,\n",
    "          run_batch_fn_eval) -> Tuple[int, float]:\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        log_dir = os.path.join(\"runs\", args.exp_name) if args.exp_name else None\n",
    "        tb_writer = SummaryWriter(log_dir)\n",
    "        args.output_dir = log_dir\n",
    "\n",
    "    args.train_batch_size = args.per_gpu_train_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "    train_sampler = RandomSampler(train_dataset) if args.local_rank == -1 else DistributedSampler(train_dataset)\n",
    "    train_dataloader = DataLoader(\n",
    "        train_dataset,\n",
    "        sampler=train_sampler,\n",
    "        batch_size=args.train_batch_size,\n",
    "        collate_fn=train_dataset.collate_fn\n",
    "    )\n",
    "\n",
    "    t_total = len(train_dataloader) // args.gradient_accumulation_steps * args.num_train_epochs\n",
    "\n",
    "    optimizer = AdamW(model.parameters(), lr=args.learning_rate, eps=args.adam_epsilon)\n",
    "    scheduler = get_linear_schedule_with_warmup(\n",
    "        optimizer, num_warmup_steps=args.warmup_steps, num_training_steps=t_total\n",
    "    )\n",
    "\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16)\n",
    "\n",
    "    # multi-gpu training (should be after apex fp16 initialization)\n",
    "    if args.n_gpu > 1:\n",
    "        model = torch.nn.DataParallel(model)\n",
    "\n",
    "    # Distributed training (should be after apex fp16 initialization)\n",
    "    if args.local_rank != -1:\n",
    "        model = torch.nn.parallel.DistributedDataParallel(\n",
    "            model, device_ids=[args.local_rank], output_device=args.local_rank, find_unused_parameters=True\n",
    "        )\n",
    "\n",
    "    # Train!\n",
    "    global_step = 0\n",
    "    model.zero_grad()\n",
    "    train_iterator = trange(\n",
    "        0, int(args.num_train_epochs), desc=\"Epoch\", disable=args.local_rank not in [-1, 0]\n",
    "    )\n",
    "    set_seed(args)  # for reproducibility\n",
    "\n",
    "    for _ in train_iterator:\n",
    "        local_steps = 0\n",
    "        tr_loss = 0.0\n",
    "        epoch_iterator = tqdm(train_dataloader, desc=\"Iteration\", disable=args.local_rank not in [-1, 0])\n",
    "        for step, batch in enumerate(epoch_iterator):\n",
    "            model.train()\n",
    "            loss, _, _, _ = run_batch_fn_train(args, model, batch)\n",
    "\n",
    "            if args.n_gpu > 1:\n",
    "                loss = loss.mean()  # mean() to average on multi-gpu parallel training\n",
    "            if args.gradient_accumulation_steps > 1:\n",
    "                loss = loss / args.gradient_accumulation_steps\n",
    "\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "\n",
    "            tr_loss += loss.item()\n",
    "\n",
    "            if (step + 1) % args.gradient_accumulation_steps == 0:\n",
    "                if args.fp16:\n",
    "                    torch.nn.utils.clip_grad_norm_(amp.master_params(optimizer), args.max_grad_norm)\n",
    "                else:\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), args.max_grad_norm)\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                optimizer.zero_grad()\n",
    "                global_step += 1\n",
    "                local_steps += 1\n",
    "                epoch_iterator.set_postfix(Loss=tr_loss / local_steps)\n",
    "\n",
    "        results = evaluate(args, eval_dataset, model, tokenizer, run_batch_fn_eval, desc=str(global_step))\n",
    "        if args.local_rank in [-1, 0]:\n",
    "            for key, value in results.items():\n",
    "                tb_writer.add_scalar(\"eval_{}\".format(key), value, global_step)\n",
    "            tb_writer.add_scalar(\"lr\", scheduler.get_lr()[0], global_step)\n",
    "            tb_writer.add_scalar(\"loss\", tr_loss / local_steps, global_step)\n",
    "\n",
    "            checkpoint_prefix = \"checkpoint\"\n",
    "            # Save model checkpoint\n",
    "            output_dir = os.path.join(args.output_dir, \"{}-{}-val_loss_{}\".format(checkpoint_prefix, global_step,\n",
    "                                                                                  results.get('loss', None)))\n",
    "            os.makedirs(output_dir, exist_ok=True)\n",
    "            model_to_save = (\n",
    "                model.module if hasattr(model, \"module\") else model\n",
    "            )  # Take care of distributed/parallel training\n",
    "\n",
    "            logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "            model_to_save.save_pretrained(output_dir)\n",
    "            tokenizer.save_pretrained(output_dir)\n",
    "\n",
    "            torch.save(args, os.path.join(output_dir, \"training_args.bin\"))\n",
    "            with open(os.path.join(output_dir, \"params.json\"), \"w\") as jsonfile:\n",
    "                json.dump(args.params, jsonfile, indent=2, default=lambda x: str(x))\n",
    "            logger.info(\"Saving model checkpoint to %s\", output_dir)\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        tb_writer.close()\n",
    "\n",
    "    return global_step, tr_loss / local_steps\n",
    "\n",
    "\n",
    "def evaluate(args, eval_dataset, model: PreTrainedModel, tokenizer: PreTrainedTokenizer, run_batch_fn, desc=\"\") -> Dict:\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        eval_output_dir = args.output_dir\n",
    "        os.makedirs(eval_output_dir, exist_ok=True)\n",
    "\n",
    "    # eval_batch_size for selection must be 1 to handle variable number of candidates\n",
    "    if args.task == \"selection\":\n",
    "        args.eval_batch_size = 1\n",
    "    else:\n",
    "        args.eval_batch_size = args.per_gpu_eval_batch_size * max(1, args.n_gpu)\n",
    "\n",
    "    eval_sampler = SequentialSampler(eval_dataset)\n",
    "    eval_dataloader = DataLoader(\n",
    "        eval_dataset,\n",
    "        sampler=eval_sampler,\n",
    "        batch_size=args.eval_batch_size,\n",
    "        collate_fn=eval_dataset.collate_fn\n",
    "    )\n",
    "\n",
    "    # multi-gpu evaluate\n",
    "    if args.n_gpu > 1 and (args.task != \"selection\" or eval_dataset.args.eval_all_snippets):\n",
    "        if not isinstance(model, torch.nn.DataParallel):\n",
    "            model = torch.nn.DataParallel(model)\n",
    "\n",
    "    eval_loss = 0.0\n",
    "    nb_eval_steps = 0\n",
    "    model.eval()\n",
    "    data_infos = []\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    print(\"eval dateloader n_bacth is \", len(eval_dataloader))\n",
    "    print(\"eval_dataset size is \",len(eval_dataset))\n",
    "    for batch in tqdm(eval_dataloader, desc=\"Evaluating\", disable=args.local_rank not in [-1, 0]):\n",
    "        with torch.no_grad():\n",
    "            loss, lm_logits, mc_logits, mc_labels = run_batch_fn(args, model, batch)\n",
    "            if args.task == \"detection\":\n",
    "                mc_logits = mc_logits.sigmoid()\n",
    "            if args.task in [\"selection\", \"detection\"]:\n",
    "                data_infos.append(batch[-1])\n",
    "            all_preds.append(mc_logits.detach().cpu().numpy())\n",
    "            all_labels.append(mc_labels.detach().cpu().numpy())\n",
    "            eval_loss += loss.mean().item()\n",
    "        nb_eval_steps += 1\n",
    "\n",
    "    eval_loss = eval_loss / nb_eval_steps\n",
    "\n",
    "    if args.task.lower() == \"generation\":\n",
    "        perplexity = torch.exp(torch.tensor(eval_loss))\n",
    "        result = {\"perplexity\": perplexity, \"loss\": eval_loss}\n",
    "    elif args.task.lower() == \"selection\":\n",
    "        all_labels = np.array(all_labels).reshape(-1)\n",
    "        all_pred_ids = np.array([np.argmax(logits) for logits in all_preds])\n",
    "        accuracy = np.sum(all_pred_ids == all_labels) / len(all_labels)\n",
    "        logger.info(\"Avg. # of candidates: %f\", sum([len(arr[0]) for arr in all_preds]) / len(all_preds))\n",
    "        result = {\"loss\": eval_loss, \"accuracy\": accuracy}\n",
    "        if args.output_file:\n",
    "            sorted_pred_ids = [np.argsort(logits.squeeze())[::-1] for logits in all_preds]\n",
    "            write_selection_preds(eval_dataset.dataset_walker, args.output_file, data_infos, sorted_pred_ids, topk=5)\n",
    "    elif args.task.lower() == \"detection\":\n",
    "        all_labels = np.concatenate(all_labels)\n",
    "        all_pred_ids = (np.concatenate(all_preds) > 0.5)\n",
    "        accuracy = np.sum(all_pred_ids == all_labels) / len(all_labels)\n",
    "        precision = sklearn.metrics.precision_score(all_labels, all_pred_ids)\n",
    "        recall = sklearn.metrics.recall_score(all_labels, all_pred_ids)\n",
    "        result = {\"loss\": eval_loss, \"accuracy\": accuracy, \"precision\": precision, \"recall\": recall}\n",
    "        if args.output_file:\n",
    "            write_detection_preds(eval_dataset.dataset_walker, args.output_file, data_infos, all_pred_ids)\n",
    "    else:\n",
    "        raise ValueError(\"args.task not in ['generation', 'selection', 'detection'], got %s\" % args.task)\n",
    "\n",
    "    if args.local_rank in [-1, 0]:\n",
    "        output_eval_file = os.path.join(eval_output_dir, \"eval_results.txt\")\n",
    "        with open(output_eval_file, \"a\") as writer:\n",
    "            logger.info(\"***** Eval results %s *****\" % desc)\n",
    "            writer.write(\"***** Eval results %s *****\\n\" % desc)\n",
    "            for key in sorted(result.keys()):\n",
    "                logger.info(\"  %s = %s\", key, str(result[key]))\n",
    "                writer.write(\"%s = %s\\n\" % (key, str(result[key])))\n",
    "\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10520c0f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
