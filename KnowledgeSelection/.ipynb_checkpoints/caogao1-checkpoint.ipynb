{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a10c6cb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig,AutoModel,AutoTokenizer\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e47abd12",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at caogao_model/best_intent_model/ were not used when initializing BertModel: ['embeddings.word_embeddings.wte.weight', 'embeddings.word_embeddings.learned_embedding']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at caogao_model/best_intent_model/ and are newly initialized: ['embeddings.word_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_path = '../pretrain_model/chinese-roberta-base/'\n",
    "config = AutoConfig.from_pretrained(model_path)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model = AutoModel.from_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fd1b0c32",
   "metadata": {},
   "outputs": [],
   "source": [
    "for name,param in model.named_parameters():\n",
    "    if 'learned_embedding' in name:\n",
    "        print(name)\n",
    "        print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3b28c5e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftEmbedding(nn.Module):\n",
    "    def __init__(self, \n",
    "                wte: nn.Embedding,\n",
    "                n_tokens: int = 10, \n",
    "                random_range: float = 0.5,\n",
    "                initialize_from_vocab: bool = True):\n",
    "        \"\"\"appends learned embedding to \n",
    "        Args:\n",
    "            wte (nn.Embedding): original transformer word embedding\n",
    "            n_tokens (int, optional): number of tokens for task. Defaults to 10.\n",
    "            random_range (float, optional): range to init embedding (if not initialize from vocab). Defaults to 0.5.\n",
    "            initialize_from_vocab (bool, optional): initalizes from default vocab. Defaults to True.\n",
    "        \"\"\"\n",
    "        super(SoftEmbedding, self).__init__()\n",
    "        self.wte = wte\n",
    "        self.n_tokens = n_tokens\n",
    "        self.learned_embedding = nn.parameter.Parameter(self.initialize_embedding(wte,\n",
    "                                                                               n_tokens, \n",
    "                                                                               random_range, \n",
    "                                                                               initialize_from_vocab))\n",
    "            \n",
    "    def initialize_embedding(self, \n",
    "                             wte: nn.Embedding,\n",
    "                             n_tokens: int = 10, \n",
    "                             random_range: float = 0.5, \n",
    "                             initialize_from_vocab: bool = True):\n",
    "        \"\"\"initializes learned embedding\n",
    "        Args:\n",
    "            same as __init__\n",
    "        Returns:\n",
    "            torch.float: initialized using original schemes\n",
    "        \"\"\"\n",
    "        if initialize_from_vocab:\n",
    "            return self.wte.weight[:n_tokens].clone().detach()\n",
    "        return torch.FloatTensor(n_tokens, wte.weight.size(1)).uniform_(-random_range, random_range)\n",
    "            \n",
    "    def forward(self, tokens):\n",
    "        \"\"\"run forward pass\n",
    "        Args:\n",
    "            tokens (torch.long): input tokens before encoding\n",
    "        Returns:\n",
    "            torch.float: encoding of text concatenated with learned task specifc embedding\n",
    "        \"\"\"\n",
    "        input_embedding = self.wte(tokens[:, self.n_tokens:])\n",
    "        learned_embedding = self.learned_embedding.repeat(input_embedding.size(0), 1, 1)\n",
    "        return torch.cat([learned_embedding, input_embedding], 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5edc0ff8",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_tokens = 20\n",
    "initialize_from_vocab = True\n",
    "\n",
    "s_wte = SoftEmbedding(model.get_input_embeddings(), \n",
    "                      n_tokens=n_tokens, \n",
    "                      initialize_from_vocab=initialize_from_vocab)\n",
    "model.set_input_embeddings(s_wte)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8fab1f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_model_path ='caogao_model/best_model.pt'\n",
    "\n",
    "model.load_state_dict(torch.load(save_model_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c49896fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "embeddings.word_embeddings.learned_embedding\n",
      "torch.Size([20, 768])\n"
     ]
    }
   ],
   "source": [
    "for name,param in model.named_parameters():\n",
    "    if 'learned_embedding' in name:\n",
    "        print(name)\n",
    "        print(param.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01fa063e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
