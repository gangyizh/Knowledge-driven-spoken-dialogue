{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61dc52f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "import jieba.posseg as pseg\n",
    "import numpy as np\n",
    "import json\n",
    "import argparse\n",
    "from collections import Counter\n",
    "import copy\n",
    "import re\n",
    "from NameEntityRecognition.ner_infere import NERInfere\n",
    "# from IntentBinExtraction.extractor_infere import IntentInfere\n",
    "from EntitySelection.entselect_infere import EntInfere\n",
    "# from AttrExtraction.extractor_infere import AttrInfere\n",
    "from IntentBinExtraction.extractor_infere import IntentInfere\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import sys\n",
    "import Levenshtein\n",
    "import itertools\n",
    "from rank_bm25 import BM25Okapi\n",
    "from itertools import chain\n",
    "from collections import Counter\n",
    "from utils import transform_attrname2inputname, transform_inputname2attrname\n",
    "from utils import load_kb, get_tail_kb, load_filter_kb,attrs2str,is_entity_str,kb_completion\n",
    "from utils import is_chinese_str,is_english_str,ch2pyinstr,get_pyin2ch\n",
    "from utils import CWDFinder,CHP_NAME,QuestionMatch,get_attrname2entities\n",
    "from utils import pre_process_query_snet,get_multi_query\n",
    "# from utils import inputname2synonyms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cca17ee7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def get_forbid_entity(kb):\n",
    "    entities = list(kb.keys())\n",
    "    attrnames = list(chain(*[list(kb[t].keys()) for t in kb.keys() ]))\n",
    "    attrnames = set(attrnames)\n",
    "    ea_entities = set(entities)&attrnames\n",
    "    return ea_entities\n",
    "\n",
    "def is_char_in_sent(w, sent):\n",
    "    flag = True\n",
    "    for c in w:\n",
    "        if c not in sent:\n",
    "            flag = False\n",
    "            break\n",
    "    return flag\n",
    "\n",
    "def is_char_pyin_in_sent(w, sent):\n",
    "    flag = True\n",
    "    for c in w:\n",
    "        if c not in sent:\n",
    "            flag = False\n",
    "            break\n",
    "            \n",
    "    if flag==False:\n",
    "        sent_pyin_str = ch2pyinstr(sent)\n",
    "        w_pyin_str = ch2pyinstr(w)\n",
    "        if w_pyin_str in sent_pyin_str:\n",
    "            flag = True\n",
    "    return flag\n",
    "\n",
    "\n",
    "def count_miss_char(w, sent):\n",
    "    cnt = 0\n",
    "    for c in w:\n",
    "        if c in sent:\n",
    "            cnt +=1\n",
    "    n_miss = len(w) - cnt\n",
    "    return n_miss\n",
    "\n",
    "def is_cnt_char_in_sent(w, query, sent):\n",
    "    cnt = 0\n",
    "    for c in w:\n",
    "        if c in sent:\n",
    "            cnt +=1\n",
    "    miss_threshold = (len(query)-1)//4 + 1\n",
    "    n_miss = len(w) - cnt\n",
    "    flag = True if n_miss<=miss_threshold else False\n",
    "    return flag\n",
    "\n",
    "def get_mapping_entiies(entities, kb, entity_mapping):\n",
    "    all_entities = []\n",
    "\n",
    "    for i, entity in enumerate(entities):\n",
    "        if entity in kb.keys():\n",
    "            all_entities.append(entity)\n",
    "\n",
    "        if entity in entity_mapping:\n",
    "            for x in entity_mapping.get(entity):\n",
    "                if x in kb.keys():\n",
    "                    all_entities.append(x)\n",
    "    return  all_entities\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "16fa77d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of kb: 38054\n"
     ]
    }
   ],
   "source": [
    "kb, entity_mapping = load_filter_kb('../data/final_data/new_kg.json')\n",
    "\n",
    "filter_entities = get_forbid_entity(kb)\n",
    "\n",
    "inputname2synonyms = {\n",
    "    \"开放什么游玩时间\": [\"什么时间\",\"什么时候\", \"开放时间\",\"几点\"],\n",
    "    \"参观游玩多久时长\": [\"建议游玩时间\",\"多久\",\"预备\"],\n",
    "    \"介绍\":[\"特征特点\",\"模样\",\"时候\"],   ### 只和生物有关\n",
    "    \"主要成就\": [\"有名气\"],\n",
    "    \"诗词全文第一句最后一句背诵\":[\"诗词全文\",\"全文\",\"第一句\",\"最后一句\",\"背诵\"],\n",
    "    \"评分\":[\"评价\"],\n",
    "    \"出生日期\":[\"岁数\", \"年纪\"],\n",
    "    \"中心思想\":[\"情感\"],\n",
    "    \"作品赏析\":[\"点评\",\"评价\"],\n",
    "    \"赏析\":[\"点评\",\"评价\"],\n",
    "    \"朝代\":[\"时候\"],\n",
    "    \"作者简介\":[\"生平\"],\n",
    "    \"外文名\":[\"原始名\"],\n",
    "    # \"简介\":[\"生平\"]\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "be0f7bd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KnowledgeSelection():\n",
    "    def __init__(self, args):\n",
    "        self.args = args\n",
    "        self.kb, self.entity_mapping = load_filter_kb(args.kb_file)\n",
    "        self.extra_kb, self.extra_entity_mapping = load_kb(args.extra_kb_file)\n",
    "        self.kb = kb_completion(self.kb, self.extra_kb)\n",
    "        self.tail_kb = get_tail_kb(self.kb, self.entity_mapping)\n",
    "        self.all_kb = {**self.kb, **self.tail_kb}\n",
    "        self.attrname2entities = get_attrname2entities(self.kb)\n",
    "        self.pyin2ch =get_pyin2ch(self.kb, self.entity_mapping, self.tail_kb)\n",
    "        self.chp_jude = CHP_NAME()\n",
    "        self.qa_macth = QuestionMatch(inputfile='../data/final_data/Noise-free/train.json')\n",
    "        \n",
    "        self.ner_infere = NERInfere(args.gpu, args.tag_file,\n",
    "                                    args.ner_pretrain_model_path,\n",
    "                                    args.ner_save_model_path,\n",
    "                                    args.ner_max_seq_len)\n",
    "        \n",
    "        self.ent_infere = EntInfere(args.gpu,\n",
    "                                      args.ent_select_pretrain_model_path,\n",
    "                                      args.ent_select_save_model_path,\n",
    "                                      args.max_conv_seq_len,\n",
    "                                      args.max_entity_len,\n",
    "                                      args.max_attrname_len,\n",
    "                                      args.max_attrvalue_len,\n",
    "                                         )\n",
    "        \n",
    "        self.attr_infere = IntentInfere(args.gpu,\n",
    "                                          args.attr_extract_pretrain_model_path,\n",
    "                                          args.attr_extract_save_model_path,\n",
    "                                          args.max_seq_len,\n",
    "                                          # args.max_entity_len,\n",
    "                                          args.max_attrname_len,\n",
    "                                          args.max_attrvalue_len,\n",
    "                                         )\n",
    "        \n",
    "        \n",
    "        self.kb_enitites = list(self.kb.keys())\n",
    "        self.tail_kb_enitites = list(self.tail_kb.keys())\n",
    "        \n",
    "        self.all_kb_entity = list(self.kb.keys()|self.entity_mapping.keys()|self.tail_kb.keys())\n",
    "        self.idf = {}\n",
    "        \n",
    "        self.tokenized_corpus = [w for w in self.all_kb_entity]\n",
    "        self.bm25 = BM25Okapi(self.tokenized_corpus)\n",
    "        self.cwdfinder =  CWDFinder(self.kb,)\n",
    "        for word in list(self.kb.keys()):\n",
    "            jieba.add_word(word, 100, \"entity\")\n",
    "\n",
    "    def get_idf(self, sentences):\n",
    "        idf = Counter()\n",
    "        for sent in sentences:\n",
    "            words = jieba.lcut(sent)\n",
    "            words = list(set(words))\n",
    "            idf.update(words)\n",
    "        for key in idf:\n",
    "            idf[key] = len(sentences) / idf[key]\n",
    "        return idf\n",
    "\n",
    "    def load_valid_data(self, valid_file):\n",
    "        with open(valid_file, 'r', encoding='utf-8') as fin:\n",
    "            data = json.load(fin)\n",
    "\n",
    "        samples = []\n",
    "        all_messages = []\n",
    "        for sample in data:\n",
    "            messages = sample.get(\"messages\")\n",
    "            previous_message = messages[0].get(\"message\")\n",
    "            all_messages.append(previous_message)\n",
    "            context = [previous_message]\n",
    "            prev_entities = []\n",
    "            for i in range(1, len(messages)):\n",
    "                message = messages[i].get(\"message\")\n",
    "                all_messages.append(message)\n",
    "                if \"attrs\" in messages[i]:\n",
    "                    attrs = messages[i].get(\"attrs\")\n",
    "                    qsample = dict(question=previous_message, answer=message,\n",
    "                                   knowledge=attrs, context=copy.deepcopy(context),\n",
    "                                   prev_entities=list(set(prev_entities)))\n",
    "                    if previous_message.endswith(\"？\"):\n",
    "                        samples.append(qsample)\n",
    "                    prev_entities.extend([attr.get(\"name\") for attr in attrs])\n",
    "                context.append(message)\n",
    "                previous_message = message\n",
    "        self.idf = self.get_idf(all_messages)\n",
    "        return samples\n",
    "\n",
    "    def load_test_data(self, test_file):\n",
    "        with open(test_file, 'r', encoding='utf-8') as fin:\n",
    "            data = json.load(fin)\n",
    "\n",
    "        samples = {}\n",
    "        all_messages = []\n",
    "        for index in data:\n",
    "            if len(data[index])==0:\n",
    "                break\n",
    "            question = data[index][-1].get(\"message\")\n",
    "            context = [turn[\"message\"] for turn in data[index]]\n",
    "            all_messages.extend(context)\n",
    "            sample = {\"question\": question, \"context\": context}\n",
    "            samples[index] = sample\n",
    "        self.idf = self.get_idf(all_messages)\n",
    "        return samples\n",
    "\n",
    "    def get_entity_by_jieba(self, context):\n",
    "        candidates = []\n",
    "        for seq in context:\n",
    "            seq = seq.replace(\"。\",\"\")\n",
    "            words = pseg.cut(seq)\n",
    "            # print(words)\n",
    "            for (word, pos) in words:\n",
    "                if pos == \"entity\":\n",
    "                    candidates.append(word)\n",
    "\n",
    "        pred_words = {}\n",
    "        for word in candidates:\n",
    "            if word not in self.all_kb_entity:\n",
    "                continue\n",
    "            s = self.idf.get(word, 5)\n",
    "            pred_words[word] = s\n",
    "\n",
    "        pred_words = dict(\n",
    "            sorted(pred_words.items(), key=lambda x: x[1], reverse=True))\n",
    "        return list(pred_words.keys())[:1]\n",
    "    \n",
    "    def get_multi_entity_by_jieba(self, context):\n",
    "        candidates = []\n",
    "        for seq in context:\n",
    "            seq = seq.replace(\"。\",\"\")\n",
    "            if len(seq)>=40:\n",
    "                continue\n",
    "            words = pseg.cut(seq)\n",
    "            # print(words)\n",
    "            for (word, pos) in words:\n",
    "                if pos == \"entity\":\n",
    "                    candidates.append(word)\n",
    "\n",
    "        candidates = [x for x in candidates if is_entity_str(x) and len(x)>=2]\n",
    "        candidates = list(set(candidates))\n",
    "        \n",
    "        return candidates\n",
    "    \n",
    "    \n",
    "    def get_bm25_match(self, query, sent, context, cidx, threshold=5):\n",
    "        scores = self.bm25.get_scores(query)\n",
    "        best_docs = sorted(list(zip(self.tokenized_corpus, scores)), key=lambda x: x[1], reverse=True)[:10]\n",
    "        \n",
    "        \n",
    "        match = []\n",
    "        ### 相同名字\n",
    "        if is_chinese_str(query):\n",
    "            pyin_str = ch2pyinstr(query)\n",
    "            pyin_macth_words = self.pyin2ch.get(pyin_str, [])\n",
    "            match.extend(pyin_macth_words)\n",
    "            if self.args.debug and len(pyin_macth_words)>0:\n",
    "                print(\"pyin match:\", pyin_macth_words)\n",
    "            if len(pyin_macth_words)==1 and len(pyin_macth_words[0])>=4:  ### 严格语音纠正\n",
    "                context[cidx] = context[cidx].replace(query, pyin_macth_words[0])\n",
    "                if self.args.debug:\n",
    "                    print(f\"pyin correct: {sent} => {context[cidx]}\")\n",
    "                    \n",
    "        ### 多字的情况\n",
    "        for (w,s) in best_docs:\n",
    "            if w in sent and len(w)<len(query):   ### todo: 考虑候选的匹配度\n",
    "                match.append(w)\n",
    "                break\n",
    "        \n",
    "        ### 缺字的情况,若多个满足筛选条件，则不考虑\n",
    "        miss_words = []\n",
    "        for (w,s) in best_docs:\n",
    "            if is_cnt_char_in_sent(w, query, sent) and len(w)>len(query) and is_entity_str(w):  \n",
    "                miss_words.append(w)\n",
    "        if self.args.debug:\n",
    "            print(\"miss_words:\", miss_words)\n",
    "        if len(miss_words)>=1:\n",
    "            miss_word_count = [(w, count_miss_char(w,sent)) for w in miss_words]\n",
    "            miss_word_count = sorted(miss_word_count, key=lambda x: x[1], )\n",
    "            match.append(miss_word_count[0][0])\n",
    "        \n",
    "        ### 其他情况\n",
    "        if best_docs[0][1]>best_docs[1][1]+ threshold: # \n",
    "            match.append(best_docs[0][0])\n",
    "            \n",
    "        match = list(set(match))\n",
    "        \n",
    "        ### 包含关系过滤\n",
    "        match = sorted(match, key=lambda x: len(x), reverse=True)\n",
    "        filter_macth = []\n",
    "        \n",
    "        for i in range(len(match)):\n",
    "            flag = True\n",
    "            for j in range(0,i):\n",
    "                if match[j] in sent and match[i] in match[j]:\n",
    "                    flag = False\n",
    "                    break\n",
    "            if flag:\n",
    "                filter_macth.append(match[i])\n",
    "        \n",
    "        if len(filter_macth)>0:\n",
    "            return filter_macth\n",
    "        else:\n",
    "            return None\n",
    "\n",
    "    def get_last_bm25_match(self, query, sent, context, cidx, threshold=0):\n",
    "        scores = self.bm25.get_scores(query)\n",
    "        best_docs = sorted(list(zip(self.tokenized_corpus, scores)), key=lambda x: x[1], reverse=True)[:10]\n",
    "        match = [x[0] for x in best_docs[:5]]\n",
    "        match = list(set(match))\n",
    "        if len(match)>0:\n",
    "            return match\n",
    "        else:\n",
    "            return None\n",
    "    \n",
    "    def get_bm25_better_word(self, word, sent, n_cand=15):\n",
    "        scores = self.bm25.get_scores(word)\n",
    "        best_words = sorted(list(zip(self.tokenized_corpus, scores)), key=lambda x: x[1], reverse=True)[1:n_cand+1]\n",
    "        best_words = [t[0] for t in best_words]\n",
    "        for w in best_words:\n",
    "            # if is_char_in_sent(w, sent) and len(w)>len(word):\n",
    "            # if is_cnt_char_in_sent(w, word, sent) and len(w)>len(word):\n",
    "            if is_char_pyin_in_sent(w, sent) and len(w)>len(word):\n",
    "                return w        \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def get_entities_bm25_tag(self, context):\n",
    "        entities = []           # 整段对话的实体\n",
    "        all_pred_entities = []  \n",
    "        question_entities = []  # 最后文本的实体   暂时未用到\n",
    "        \n",
    "        ### 遍历整段对话，识别实体\n",
    "        for cidx,sent in enumerate(context):\n",
    "#             if cidx%2==1 and cidx>1:\n",
    "#                 continue\n",
    "            ### todo: 写好符号列表\n",
    "            sent = sent.replace('。','')\n",
    "            pred_entities = self.ner_infere.ner(sent)\n",
    "            if args.debug:\n",
    "                print(sent, pred_entities)\n",
    "            if len(pred_entities)==1 and is_english_str(pred_entities[0]):\n",
    "                pred_entities = re.findall('[a-zA-Z]+', sent)\n",
    "            \n",
    "            for pred in pred_entities:\n",
    "                if pred == \"\" or pred in self.attrname2entities:\n",
    "                    continue\n",
    "                pred = pred.replace('。', '')\n",
    "                if pred in self.all_kb_entity:\n",
    "                    better_word = self.get_bm25_better_word(pred, sent)\n",
    "                    if better_word is not None:\n",
    "                        if pred in better_word:\n",
    "                            pred = better_word\n",
    "                        else:\n",
    "                            entities.append(better_word)\n",
    "                        \n",
    "                    if pred in filter_entities:\n",
    "                        continue\n",
    "                        \n",
    "                    entities.append(pred)\n",
    "                    \n",
    "                    if cidx == len(context) - 1:\n",
    "                        question_entities.append(pred)\n",
    "                    # print(pred)\n",
    "                elif len(pred)>=2:\n",
    "                    macth_pred = self.get_bm25_match(pred, sent, context, cidx, threshold=self.args.bm25_threshold)\n",
    "                    if macth_pred is not None:\n",
    "                        entities.extend(macth_pred)\n",
    "                        if cidx ==len(context) - 1:\n",
    "                            question_entities.extend(macth_pred)\n",
    "                all_pred_entities.append(pred)\n",
    "        \n",
    "        entities = [t for t in entities if is_entity_str(t)]\n",
    "        question_entities = [t for t in question_entities if is_entity_str(t)]\n",
    "        \n",
    "        entities2count = Counter(entities)\n",
    "        all_pred_entities = list(set(all_pred_entities))     # 加上实体是 数字字符串 直接删除\n",
    "        \n",
    "        \n",
    "        r_entities = []\n",
    "        for t in range(len(entities)-1, -1, -1):\n",
    "            if entities[t] not in r_entities:\n",
    "                r_entities.append(entities[t])\n",
    "        question_entities = list(set(question_entities))\n",
    "        \n",
    "        ### 最后问题的实体被提起两次则直接该实体，\n",
    "        ### 加进去效果很差，一些短语干扰，所以将实体长度改成大于等于4，提高筛选\n",
    "#         if len(question_entities)!=0:\n",
    "#             hard_entities = []\n",
    "#             for t in question_entities:\n",
    "#                 if entities2count.get(t)>=2 and len(t)>=3:  \n",
    "#                     hard_entities.append(t)\n",
    "#             if len(hard_entities)>0:\n",
    "#                 return hard_entities, all_pred_entities, \"last_hard_ner\"\n",
    "\n",
    "\n",
    "#         if len(entities) == 0:\n",
    "#             r_entities = []\n",
    "#             jieba_entities = self.get_entity_by_jieba(context)\n",
    "#             r_entities.extend(jieba_entities)\n",
    "            \n",
    "#             return r_entities, all_pred_entities, entities2count, question_entities,\"jieba&&bm25\"\n",
    "#         else:\n",
    "#             return r_entities, all_pred_entities, entities2count, question_entities, \"ner\"\n",
    "        \n",
    "        jieba_entities = self.get_multi_entity_by_jieba(context)\n",
    "        if args.debug:\n",
    "            print(\"jieba_entities:\", jieba_entities)\n",
    "        if len(jieba_entities)>0:\n",
    "            for x in jieba_entities:\n",
    "#                 if x not in r_entities:\n",
    "#                     r_entities.append(x)\n",
    "                flag = True\n",
    "                for ent in r_entities:\n",
    "                    if x in ent:\n",
    "                        flag = False\n",
    "                        break\n",
    "                if flag:\n",
    "                    r_entities.append(x)\n",
    "        \n",
    "        return r_entities, all_pred_entities, entities2count, question_entities, \"ner&&jibea\"\n",
    "\n",
    "        \n",
    "    def get_last_entities_bm25_tag(self, context):\n",
    "        entities = []           # 整段对话的实体\n",
    "        all_pred_entities = []  \n",
    "        question_entities = []  # 最后文本的实体   暂时未用到\n",
    "        \n",
    "        ### 遍历整段对话，识别实体\n",
    "        for cidx,sent in enumerate(context):\n",
    "            sent = sent.replace('。','')\n",
    "            pred_entities = self.ner_infere.ner(sent)\n",
    "            if args.debug:\n",
    "                print(sent, pred_entities)\n",
    "            \n",
    "            for pred in pred_entities:\n",
    "                if pred == \"\" or pred in self.attrname2entities:\n",
    "                    continue\n",
    "                pred = pred.replace('。', '')\n",
    "                if len(pred)>=2:\n",
    "                    macth_pred = self.get_last_bm25_match(pred, sent, context, cidx)\n",
    "                    if macth_pred is not None:\n",
    "                        entities.extend(macth_pred)\n",
    "                        if cidx ==len(context) - 1:\n",
    "                            question_entities.extend(macth_pred)\n",
    "                all_pred_entities.append(pred)\n",
    "        \n",
    "        entities = [t for t in entities if is_entity_str(t)]\n",
    "        question_entities = [t for t in question_entities if is_entity_str(t)]\n",
    "        entities2count = Counter(entities)\n",
    "        all_pred_entities = list(set(all_pred_entities))     # 加上实体是 数字字符串 直接删除\n",
    "        r_entities = []\n",
    "        for t in range(len(entities)-1, -1, -1):\n",
    "            if entities[t] not in r_entities:\n",
    "                r_entities.append(entities[t])\n",
    "        question_entities = list(set(question_entities))\n",
    "\n",
    "        return r_entities, all_pred_entities, entities2count, question_entities,\"last_bm25\"\n",
    "    \n",
    "#     def entities_attr_filter(self, all_entities, question):    ### 这个过滤方法，会过滤掉正确答案，应该更严格设计\n",
    "#         ### 按实体属性有无在问题中过滤，如果过滤后列表为空，则返回原列表\n",
    "#         filter_entities = []\n",
    "#         for entity in all_entities:\n",
    "#             flag = False\n",
    "#             for attr in self.all_kb[entity]:\n",
    "#                 if attr:\n",
    "#                     flag = True\n",
    "#                     break\n",
    "#             if flag:\n",
    "#                 filter_entities.append(entity)\n",
    "                \n",
    "#         if len(filter_entities)>0:                \n",
    "#             return filter_entities\n",
    "#         else:\n",
    "#             return all_entities\n",
    "        \n",
    "    \n",
    "    def get_ent_attr_intent(self, entities, query, context):\n",
    "        if len(entities)==0:\n",
    "            return None, None\n",
    "        query = query.replace(\"。\",\"\")\n",
    "        query = pre_process_query_snet(query)\n",
    "        all_attrnames = []\n",
    "        for ent in entities:\n",
    "            attrnames,attrvalues = attrs2str(self.all_kb, ent)\n",
    "            all_attrnames.append(attrnames)\n",
    "        \n",
    "        pred_ent_score = self.ent_infere.text_smiliary([context]*len(entities), entities, all_attrnames, [\"\"]*len(entities),bacth_size = 4)\n",
    "        pred_ent_score = pred_ent_score.squeeze(-1)\n",
    "        ent_index = np.argmax(pred_ent_score)\n",
    "        pred_entities = entities[ent_index]\n",
    "        select_entities = [pred_entities]\n",
    "        \n",
    "        ### 强插机制\n",
    "        for ent in entities:\n",
    "            if ent in query and ent not in select_entities and is_chinese_str(ent) and ent not in pred_entities:\n",
    "                if len(ent)>=3 or self.chp_jude.is_people_name(ent): \n",
    "                    select_entities.append(ent)\n",
    "        if len(select_entities)>1: \n",
    "            select_entities = select_entities[::-1]\n",
    "        \n",
    "        ### ()问题\n",
    "        if select_entities[0] in self.entity_mapping:\n",
    "            for x in self.entity_mapping[select_entities[0]]:\n",
    "                select_entities.append(x)\n",
    "#        如果实体属性在问题中出现，将实体加入候选\n",
    "#         attr_in_query_entities = []\n",
    "#         for ent in entities:\n",
    "#             flag = False\n",
    "#             for attr in self.all_kb[ent]:\n",
    "#                 if attr in query:\n",
    "#                     flag = True\n",
    "#             if flag:\n",
    "#                 attr_in_query_entities.append(ent)\n",
    "#         if pred_entities not in attr_in_query_entities:\n",
    "#             select_entities.extend(attr_in_query_entities)\n",
    "            \n",
    "#         尝试筛选2个实体进入排序            \n",
    "#         topk = 2\n",
    "#         ent_index = np.argsort(pred_ent_score.squeeze(-1))[::-1][:topk]\n",
    "#         select_entities = [entities[int(t)] for t in ent_index]\n",
    "        \n",
    "        if self.args.debug:\n",
    "            print(\"entity score:\", list(zip(entities, pred_ent_score)))\n",
    "            print(\"select_entities:\", select_entities)\n",
    "        \n",
    "        candidates = []                  # 所有实体的所有attrnames\n",
    "        input_candidates = []\n",
    "        candidates_attrvalues = []\n",
    "        global_entities = []\n",
    "        for entity in select_entities:\n",
    "#             attrs = list(self.kb.get(entity, {}).keys())\n",
    "#             candidates.extend(attrs)\n",
    "            entity_attrs = self.all_kb.get(entity, {})\n",
    "            attr_count = 0\n",
    "            for attr,attrvalue in entity_attrs.items():\n",
    "                candidates.append(attr)\n",
    "                input_candidates.append(attr)\n",
    "                attrvalue_str = ','.join([str(t) for t in attrvalue])\n",
    "                candidates_attrvalues.append(attrvalue_str)    # 这里置空了，导致效果不好\n",
    "                attr_count +=1\n",
    "                \n",
    "                if attr in inputname2synonyms:\n",
    "                    for x in inputname2synonyms[attr]:\n",
    "                        candidates.append(attr)\n",
    "                        input_candidates.append(x)\n",
    "                        candidates_attrvalues.append(attrvalue_str)\n",
    "                        attr_count +=1\n",
    "                        \n",
    "            global_entities.extend([entity] * attr_count)\n",
    "        if len(candidates) == 0:\n",
    "            return None, None\n",
    "        \n",
    "        if len(select_entities)==1:\n",
    "            query = query.replace(select_entities[0],\"ne\")      ### 在推断时同样遮盖实体\n",
    "        pred_attr_score = self.attr_infere.text_smiliary([query]*len(candidates), input_candidates, candidates_attrvalues, bacth_size = 16)\n",
    "        if args.debug:\n",
    "            print(list(zip(global_entities,candidates,pred_attr_score,candidates_attrvalues)))\n",
    "        attr_index = np.argmax(pred_attr_score)\n",
    "        pred_intent = candidates[attr_index]\n",
    "        pred_entities = global_entities[attr_index]\n",
    "        return pred_intent, pred_entities\n",
    "\n",
    "    def get_pred_knowledge(self, entity, intent):\n",
    "        if entity is None:\n",
    "            return []\n",
    "        pred_knowledge = []\n",
    "        if entity not in self.all_kb:\n",
    "            return []\n",
    "        if intent not in self.all_kb.get(entity):\n",
    "            print(f\"{intent} not in {self.kb.get(entity)}\")\n",
    "            return []\n",
    "        \n",
    "        for value in self.all_kb.get(entity)[intent]:\n",
    "#             if intent == \"简介\":\n",
    "#                 intent = \"Information\"\n",
    "            intent = transform_inputname2attrname(intent)\n",
    "            if entity in self.kb.keys():\n",
    "                known = {\"name\": entity, \"attrname\": intent, \"attrvalue\": value}\n",
    "            else:\n",
    "                known = {\"name\": value, \"attrname\": intent, \"attrvalue\": entity, \"know_reverse\":1}\n",
    "                \n",
    "            pred_knowledge.append(known)\n",
    "        return pred_knowledge\n",
    "\n",
    "    def _match(self, gold_knowledge, pred_knowledge):\n",
    "        result = []\n",
    "        for pred in pred_knowledge:\n",
    "            matched = False\n",
    "            for gold in gold_knowledge:\n",
    "                if isinstance(pred[\"attrvalue\"], list):\n",
    "                    pred_attrvalue = \" \".join(sorted(pred[\"attrvalue\"]))\n",
    "                else:\n",
    "                    pred_attrvalue = pred[\"attrvalue\"]\n",
    "                if isinstance(gold[\"attrvalue\"], list):\n",
    "                    gold_attrvalue = \" \".join(sorted(gold[\"attrvalue\"]))\n",
    "                else:\n",
    "                    gold_attrvalue = gold[\"attrvalue\"]\n",
    "                if pred['name'] == gold['name'] and pred['attrname'] == gold[\n",
    "                    'attrname'] and pred_attrvalue == gold_attrvalue:\n",
    "                    matched = True\n",
    "            result.append(matched)\n",
    "        return result\n",
    "\n",
    "    def calu_knowledge_selection(self, gold_knowledge, pred_knowledge):\n",
    "        if len(gold_knowledge) == 0 and len(pred_knowledge) == 0:\n",
    "            return 1.0, 1.0, 1.0\n",
    "\n",
    "        precision, recall, f1 = 0.0, 0.0, 0.0\n",
    "        relevance = self._match(gold_knowledge, pred_knowledge)\n",
    "        if len(relevance) == 0 or sum(relevance) == 0:\n",
    "            return precision, recall, f1\n",
    "\n",
    "        tp = sum(relevance)\n",
    "        precision = tp / len(pred_knowledge) if len(\n",
    "            pred_knowledge) > 0 else 0.0\n",
    "        recall = tp / len(gold_knowledge) if len(gold_knowledge) > 0 else 0.0\n",
    "        if precision == 0 and recall == 0:\n",
    "            return precision, recall, f1\n",
    "        f1 = 2 * precision * recall / (precision + recall)\n",
    "        return precision, recall, f1\n",
    "\n",
    "    def evaluate(self, datafile, outputfile):\n",
    "        data = self.load_valid_data(datafile)\n",
    "\n",
    "        total = len(data)\n",
    "        metrics = {\"p\": 0, \"r\": 0, \"f1\": 0}\n",
    "        eval_samples = []\n",
    "        for sample in tqdm(data):\n",
    "            knowledge = sample.get(\"knowledge\")\n",
    "            context = sample.get(\"context\")\n",
    "            question = sample.get(\"question\")\n",
    "            answer = sample.get(\"answer\")\n",
    "            \n",
    "            cwd_res = self.cwdfinder.run(question)\n",
    "            ### 成语规则\n",
    "            if cwd_res is not None and len(cwd_res)>0:\n",
    "                pred_knowledges = []\n",
    "                for entity, intent in cwd_res:\n",
    "                    pred_knowledge = self.get_pred_knowledge(entity, intent)\n",
    "                    pred_knowledges.extend(pred_knowledge)\n",
    "                if len(pred_knowledges)>0:\n",
    "                    p, r, f1 = self.calu_knowledge_selection(knowledge, pred_knowledges)\n",
    "                    eval_sample = {\"question\": question, \"context\": context,\n",
    "                              \"groud_attrs\": knowledge, \"pred_attrs\":pred_knowledges,\"groud_response\":answer,\n",
    "                              \"entities\":[], \"tag\":\"cwd\", \"pred_entities\":[]}\n",
    "                    eval_samples.append(eval_sample)\n",
    "                    metrics[\"p\"] += p\n",
    "                    metrics[\"r\"] += r\n",
    "                    metrics[\"f1\"] += f1\n",
    "                    continue\n",
    "            \n",
    "            \n",
    "            entities, pred_entities, entities2count, question_entities, tag = self.get_entities_bm25_tag(context)\n",
    "            if len(entities)==0:\n",
    "                entities, pred_entities, entities2count, question_entities, tag = self.get_last_entities_bm25_tag(context)\n",
    "                \n",
    "            all_entities = get_mapping_entiies(entities, self.all_kb, self.entity_mapping)\n",
    "            all_question_entities = get_mapping_entiies(entities, self.all_kb, self.entity_mapping)\n",
    "            \n",
    "            intent, entity = self.get_ent_attr_intent(all_entities, question, context)\n",
    "            pred_knowledge = self.get_pred_knowledge(entity, intent)        # 根据 entity 和 intent查找值\n",
    "            p, r, f1 = self.calu_knowledge_selection(knowledge, pred_knowledge)\n",
    "            eval_sample = {\"question\": question, \"context\": context,\n",
    "                      \"groud_attrs\": knowledge, \"pred_attrs\":pred_knowledge,\"groud_response\":answer,\n",
    "                      \"entities\":entities, \"tag\":tag, \"pred_entities\":pred_entities}\n",
    "            eval_samples.append(eval_sample)\n",
    "            \n",
    "            metrics[\"p\"] += p\n",
    "            metrics[\"r\"] += r\n",
    "            metrics[\"f1\"] += f1\n",
    "\n",
    "        for key in metrics:\n",
    "            metrics[key] = metrics.get(key) / total\n",
    "        print(metrics)\n",
    "        \n",
    "        with open(outputfile, 'w', encoding='utf-8') as fout:\n",
    "            json.dump(eval_samples, fout, ensure_ascii=False)\n",
    "        \n",
    "        return metrics\n",
    "\n",
    "    def test(self, datafile, outputfile, test_index=None):\n",
    "        data = self.load_test_data(datafile)\n",
    "\n",
    "        samples = {}\n",
    "        for index in tqdm(data):\n",
    "            if  self.args.debug and test_index is not None and index!=str(test_index):\n",
    "                continue\n",
    "            question = data.get(index).get(\"question\")\n",
    "            context = data.get(index).get(\"context\")\n",
    "            \n",
    "            ### 历史问句匹配\n",
    "            match_attrs = self.qa_macth.run(question)\n",
    "            if len(context)==1 and match_attrs is not None:\n",
    "                sample = {\"question\": question, \"context\": context,\n",
    "                      \"attrs\": match_attrs}\n",
    "                samples[index] = sample\n",
    "                if args.debug:\n",
    "                    print(\"question match!!!\")\n",
    "                    print(\"question:\", question)\n",
    "                    print(\"pred_knowledges:\", pred_knowledges)\n",
    "                continue\n",
    "            \n",
    "            ### 成语规则\n",
    "            cwd_res = self.cwdfinder.run(question)\n",
    "            if cwd_res is not None and len(cwd_res)>0:\n",
    "                pred_knowledges = []\n",
    "                for entity, intent in cwd_res:\n",
    "                    pred_knowledge = self.get_pred_knowledge(entity, intent)\n",
    "                    pred_knowledges.extend(pred_knowledge)\n",
    "                for pred_knowledge in pred_knowledges:\n",
    "                    pred_knowledge[\"know_reverse\"] = 1\n",
    "                sample = {\"question\": question, \"context\": context,\n",
    "                          \"attrs\": pred_knowledges}\n",
    "                samples[index] = sample\n",
    "                if args.debug:\n",
    "                    print(\"question:\", question)\n",
    "                    print(\"pred_knowledges:\", pred_knowledges)\n",
    "                continue\n",
    "            \n",
    "            \n",
    "            entities, pred_entities, entities2count, question_entities, tag = self.get_entities_bm25_tag(context)\n",
    "            if len(entities)==0:\n",
    "                if args.debug:\n",
    "                    print(\"starting last_entities_bm25 match...\")\n",
    "                entities, pred_entities, entities2count, question_entities, tag = self.get_last_entities_bm25_tag(context)\n",
    "            if args.debug:\n",
    "                print(\"get_entities_bm25_tag: \", entities)\n",
    "            \n",
    "#             if len(entities)==0:\n",
    "#                 if args.debug:\n",
    "#                     print(\"starting jieba match...\")\n",
    "#                 entities, pred_entities, entities2count, question_entities, tag = self.get_last_entities_bm25_tag(context)\n",
    "#                 if args.debug:\n",
    "#                     print(\"jieba: \", entities)   \n",
    "            \n",
    "            all_entities = get_mapping_entiies(entities, self.all_kb, self.entity_mapping)\n",
    "            all_question_entities = get_mapping_entiies(entities, self.all_kb, self.entity_mapping)\n",
    "            if args.debug:\n",
    "                print(\"entity mapping: \", all_entities)\n",
    "                \n",
    "            # all_entities = self.entities_attr_filter(all_entities, question)\n",
    "            # if args.debug:\n",
    "            #     print(\"entity attr filter: \", all_entities)\n",
    "            \n",
    "            multi_query = get_multi_query(question)\n",
    "            # print(multi_query)\n",
    "            if len(multi_query)<2:\n",
    "                intent, entity = self.get_ent_attr_intent(all_entities, question, context)\n",
    "                pred_knowledge = self.get_pred_knowledge(entity, intent)\n",
    "            else:\n",
    "                pred_knowledge = []\n",
    "                pred_intent_entity = []\n",
    "                for query in multi_query:\n",
    "                    intent, entity = self.get_ent_attr_intent(all_entities, query, context)\n",
    "                    intent_entity_str = intent+\"_\"+entity\n",
    "                    # print(intent, entity)\n",
    "                    if intent_entity_str not in pred_intent_entity:\n",
    "                        pred_intent_entity.append(intent_entity_str)\n",
    "                        pred_know = self.get_pred_knowledge(entity, intent)\n",
    "                        pred_knowledge.extend(pred_know)\n",
    "            \n",
    "            \n",
    "            if args.debug:\n",
    "                print(entity, intent)\n",
    "                print(pred_knowledge)\n",
    "            sample = {\"question\": question, \"context\": context,\n",
    "                      \"attrs\": pred_knowledge, \"entities\":entities,\n",
    "                     \"tag\":tag, \"pred_entities\":pred_entities}\n",
    "            samples[index] = sample\n",
    "\n",
    "        with open(outputfile, 'w', encoding='utf-8') as fout:\n",
    "            json.dump(samples, fout, ensure_ascii=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f329dcec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length of kb: 38054\n",
      "length of kb: 1000\n",
      "tail entity count: 5761\n",
      "Load model...\n",
      "Model created!\n",
      "Load model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../pretrain_model/ernie-3.0-base-zh/ were not used when initializing ErnieModel: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.bias']\n",
      "- This IS expected if you are initializing ErnieModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing ErnieModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created!\n",
      "Load model...\n",
      "Model created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache /tmp/jieba.cache\n",
      "Loading model cost 0.617 seconds.\n",
      "Prefix dict has been built successfully.\n",
      "100%|██████████| 300/300 [02:57<00:00,  1.69it/s]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--mode', type=str,\n",
    "                    default=\"test\")\n",
    "\n",
    "parser.add_argument('--tag_file', type=str,\n",
    "                    default=\"data/tag.txt\")\n",
    "parser.add_argument('--ner_pretrain_model_path', type=str,\n",
    "                    default=\"../pretrain_model/chinese-roberta-wwm-ext\")\n",
    "parser.add_argument('--ner_save_model_path', type=str,\n",
    "                    default=\"model/ner/\")\n",
    "parser.add_argument('--gpu', type=int, default=0)\n",
    "parser.add_argument('--ner_max_seq_len', type=int, default=512)\n",
    "\n",
    "# parser.add_argument('--extractor_pretrain_model_path', type=str,\n",
    "#                     default=\"pretrain_model/ernie-1.0/\")\n",
    "# parser.add_argument('--extractor_save_model_path', type=str,\n",
    "#                     default=\"model/intent\")\n",
    "\n",
    "parser.add_argument('--ent_select_pretrain_model_path', type=str,\n",
    "                    default=\"../pretrain_model/ernie-3.0-base-zh/\")\n",
    "parser.add_argument('--ent_select_save_model_path', type=str,\n",
    "                    default=\"model/entity_select_ernie3/\")\n",
    "\n",
    "parser.add_argument('--attr_extract_pretrain_model_path', type=str,\n",
    "                    default=\"../pretrain_model/chinese-pert-base/\")\n",
    "parser.add_argument('--attr_extract_save_model_path', type=str,\n",
    "                    default=\"model/intent_retrain_1/\")\n",
    "\n",
    "parser.add_argument('--extractor_max_seq_len', type=int, default=50)\n",
    "parser.add_argument('--max_conv_seq_len', type=int, default=400)\n",
    "parser.add_argument('--max_seq_len', type=int, default=64)\n",
    "parser.add_argument('--max_entity_len', type=int, default=40)  # 多实体\n",
    "parser.add_argument('--max_attrname_len', type=int, default=40)\n",
    "parser.add_argument('--max_attrvalue_len', type=int, default=40)\n",
    "\n",
    "parser.add_argument('--bm25_threshold', type=float, default=2)\n",
    "parser.add_argument('--kb_file', type=str,\n",
    "                    default=\"../data/final_data/new_kg.json\")\n",
    "parser.add_argument('--extra_kb_file', type=str,\n",
    "                    default=\"../data/Comparison_Data/Knowledge_Graph_Data/Marine_Fishes/Modified_Data/fish_final_v2_16.json\")\n",
    "parser.add_argument('--valid_file', type=str,\n",
    "                    default=\"../data/final_data/Noise-added/valid.json\")\n",
    "parser.add_argument('--test_file', type=str,\n",
    "                    default=\"../data/final_data/test.json\")\n",
    "parser.add_argument('--result_file', type=str,\n",
    "                    default=\"test_know_result.json\")\n",
    "parser.add_argument('--val_result_file', type=str,\n",
    "                    default=\"eval_know_result.json\")\n",
    "\n",
    "parser.add_argument('--debug', type=bool,\n",
    "                    default=False)\n",
    "\n",
    "sys.argv = ['']\n",
    "args = parser.parse_args()\n",
    "\n",
    "selector = KnowledgeSelection(args)\n",
    "if args.mode == \"test\":\n",
    "    selector.test(args.test_file, args.result_file, test_index=127)\n",
    "else:\n",
    "    selector.evaluate(args.valid_file, args.val_result_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "03241b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "### {'p': 0.7276130019981182, 'r': 0.7598732603672693, 'f1': 0.7277000286985552} pert\n",
    "### {'p': 0.7332888481196234, 'r': 0.7659445137582621, 'f1': 0.7337336197542192} chinese-roberta-wwm-ext\n",
    "### {'p': 0.7360266680741765, 'r': 0.767914042372735, 'f1': 0.7357241571872623} macbert\n",
    "### {'p': 0.7352520368707036, 'r': 0.7691438090323943, 'f1': 0.7359585652942794} erniezh1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ba638f7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### {'p': 0.733349911014405, 'r': 0.7665970040050345, 'f1': 0.7337462866472727}\n",
    "### {'p': 0.7325791170966698, 'r': 0.7660013905231482, 'f1': 0.7330689222953236}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb0b49bf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5128fad8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
