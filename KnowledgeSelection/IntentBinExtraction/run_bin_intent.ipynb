{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "180650ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import argparse\n",
    "\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torch.optim.adamw import AdamW\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from extractor_model import ExtractorModel\n",
    "from extractor_dataset import DatasetExtractor\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from NameEntityRecognition.ner_infere import NERInfere\n",
    "from utils import transform_attrname2inputname, transform_inputname2attrname,seed_everything,BCEFocalLoss\n",
    "from utils import load_kb\n",
    "\n",
    "def load_data(datafile, kb):\n",
    "    data = []\n",
    "    with open(datafile, 'r', encoding='utf-8') as fin:\n",
    "        for line in fin:\n",
    "            data.append(json.loads(line.strip()))\n",
    "    print(f\"length of data: {len(data)}\")\n",
    "\n",
    "    samples = []\n",
    "    for text_id,sample in enumerate(data):\n",
    "#         if(text_id>1000):\n",
    "#             break\n",
    "        query = sample.get(\"question\")  \n",
    "        # {question: str，answer: str，knowledge: list(dict), context: list(str)，prev_entities: list(str)}\n",
    "        entity2attr = {}     # answer中使用到的 entity to set(attrname)\n",
    "        for known in sample.get(\"knowledge\"):\n",
    "            entity = known.get(\"name\")\n",
    "            attrname = known.get(\"attrname\")\n",
    "#             if attrname == \"Information\":\n",
    "#                 attrname = \"简介\"\n",
    "            attrname = transform_attrname2inputname(attrname)\n",
    "            if entity not in entity2attr:\n",
    "                entity2attr[entity] = set()\n",
    "            entity2attr.get(entity).add(attrname)\n",
    "        \n",
    "        entities = [entity for entity, attrs in entity2attr.items()]\n",
    "        entities = sorted(entities)\n",
    "        for entity in entities:\n",
    "            attrs = entity2attr[entity]\n",
    "            attrs = sorted(list(attrs))\n",
    "            subgraph = kb.get(entity, {})  # entity 对于的所有attrname\n",
    "            text1 = query.replace(entity, \"ne\")   # 不理解替换的意思\n",
    "            for attr in attrs:\n",
    "                text2 = attr\n",
    "                attrvalue =  kb.get(entity)[text2]\n",
    "                attrvalue = ','.join(attrvalue)\n",
    "                samples.append([text_id, text1, text2, attrvalue, 1])       # (id, question, attrname, attrvalue)正样本\n",
    "            for key in subgraph:\n",
    "                if key not in attrs:    # 优化点 key not in attrs\n",
    "                    text3 = key\n",
    "                    attrvalue =  kb.get(entity)[text3]\n",
    "                    attrvalue = ','.join(attrvalue)\n",
    "                    samples.append([text_id, text1, text3, attrvalue, 0])   # 同一实体的负样本\n",
    "            \n",
    "    print(f\"length of sample: {len(samples)}\")\n",
    "    return samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9ff9726c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### metric\n",
    "def hit_1(y, x):\n",
    "    idx = x.argmax()\n",
    "    if y[idx]==1:\n",
    "        return 1\n",
    "    return 0\n",
    "\n",
    "def gorup_metric_fn(df):\n",
    "    if len(np.unique(df['labels']))==2:\n",
    "        df['auc'] = roc_auc_score(df['labels'],df['logits'])\n",
    "    else:\n",
    "#         print(\"all label same:\",df['labels'])\n",
    "        df['auc']=0.5\n",
    "    \n",
    "    df['hit_1'] =  hit_1(df['labels'].values, df['logits'].values)\n",
    "    return df\n",
    "\n",
    "def cal_acc_score(ids, logits, labels):\n",
    "    df = pd.DataFrame({'ids':ids.squeeze(-1).tolist(), 'logits':logits.squeeze(-1).tolist(), 'labels':labels.squeeze(-1).tolist()})\n",
    "    df1 = df.groupby('ids',as_index=False,sort=True).apply(gorup_metric_fn)\n",
    "    df1 = df1[['ids','auc','hit_1']].drop_duplicates(ignore_index=True)\n",
    "    auc = df1['auc'].mean()\n",
    "    acc = df1['hit_1'].mean()\n",
    "    return (auc, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b9e1ce43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args):\n",
    "    pretrain_model_path = args.pretrain_model_path\n",
    "    save_model_path = args.save_model_path\n",
    "    os.makedirs(save_model_path,exist_ok=True)\n",
    "    \n",
    "    max_seq_len = args.max_seq_len\n",
    "    gpu = args.gpu\n",
    "    batch_size = args.batch_size\n",
    "    learning_rate = args.learning_rate\n",
    "    nb_epochs = args.epochs\n",
    "\n",
    "    device = torch.device(gpu)\n",
    "    print(\"Loading dataset...\")\n",
    "\n",
    "    kb, entity_mapping = load_kb(args.kb_file)                 # head to relation to list(attr)\n",
    "    train_data = load_data(args.train_file, kb)\n",
    "    train_dataset = DatasetExtractor(train_data, max_seq_len, args.max_attrname_len, args.max_attrvalue_len,\n",
    "                                     args.pretrain_model_path)\n",
    "    train_dataloader = DataLoader(train_dataset, batch_size=batch_size,\n",
    "                                  shuffle=False)   # 可以改为 shuffle=True\n",
    "    \n",
    "    dev_data = load_data(args.dev_file, kb)\n",
    "    \n",
    "    print('Creating model...')\n",
    "    model = ExtractorModel(device=device, model_path=args.pretrain_model_path)\n",
    "    print('Model created!')\n",
    "    model.to(device)\n",
    "    \n",
    "    optimizer = AdamW(model.parameters(), lr=learning_rate)\n",
    "    # loss_fn = BCEFocalLoss(gamma=2, alpha=0.75, reduction='mean')\n",
    "    loss_fn = nn.BCEWithLogitsLoss()\n",
    "    if args.fp16:\n",
    "        try:\n",
    "            from apex import amp\n",
    "        except ImportError:\n",
    "            raise ImportError(\"Please install apex from https://www.github.com/nvidia/apex to use fp16 training.\")\n",
    "        model, optimizer = amp.initialize(model, optimizer, opt_level=args.fp16_opt_level)\n",
    "    \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    best_score = -float(\"inf\")\n",
    "    not_up_epoch = 0\n",
    "    \n",
    "    model.zero_grad()\n",
    "    for epoch in range(nb_epochs):\n",
    "        model.train()\n",
    "        loader = tqdm(train_dataloader, total=len(train_dataloader),\n",
    "                      unit=\"batches\")\n",
    "        running_loss = 0\n",
    "        \n",
    "        all_ids, all_logits, all_labels= [],[],[]\n",
    "        for i_batch, data in enumerate(loader):\n",
    "            model.zero_grad()\n",
    "            text_id, inputs, label = data\n",
    "            token_ids = inputs[\"input_ids\"].flatten(1).to(device)\n",
    "            attention_mask = inputs[\"attention_mask\"].flatten(1).to(\n",
    "                device)\n",
    "            token_type_ids = inputs[\"token_type_ids\"].flatten(1).to(\n",
    "                device)\n",
    "            label = label.to(device)\n",
    "            \n",
    "            logit = model(\n",
    "                token_ids=token_ids,\n",
    "                attention_mask=attention_mask,\n",
    "                token_type_ids=token_type_ids\n",
    "            )\n",
    "            loss = loss_fn(logit, label)\n",
    "            if args.fp16:\n",
    "                with amp.scale_loss(loss, optimizer) as scaled_loss:\n",
    "                    scaled_loss.backward()\n",
    "            else:\n",
    "                loss.backward()\n",
    "            optimizer.step()\n",
    "            running_loss += loss.item()\n",
    "            loader.set_postfix(\n",
    "                Loss=running_loss / ((i_batch + 1) * batch_size),\n",
    "                Epoch=epoch)\n",
    "            loader.set_description('{}/{}'.format(epoch, nb_epochs))\n",
    "            loader.update()\n",
    "            \n",
    "            all_ids.append(text_id.detach().cpu().numpy())\n",
    "            all_logits.append(logit.detach().cpu().numpy())\n",
    "            all_labels.append(label.detach().cpu().numpy())\n",
    "        \n",
    "        all_ids = np.concatenate(all_ids,)\n",
    "        all_logits = np.concatenate(all_logits,)\n",
    "        all_labels = np.concatenate(all_labels,)\n",
    "        trn_auc, trn_acc =  cal_acc_score(all_ids, all_logits, all_labels)\n",
    "        print(\"train step %d auc=%.6f, acc=%.6f\"%(epoch+1, trn_auc, trn_acc))\n",
    "        \n",
    "        val_auc, val_acc  = validate(device, model, dev_data, args)\n",
    "        print(\"valid step %d auc=%.6f, acc=%.6f\"%(epoch+1, val_auc, val_acc))\n",
    "        score = val_acc\n",
    "        \n",
    "        if epoch==nb_epochs-1:\n",
    "            print(\"save final model for test...\")\n",
    "            torch.save(model.state_dict(),\n",
    "                       os.path.join(save_model_path, \"x.pt\"))\n",
    "        if score > best_score + 0.0001:\n",
    "            best_score = score\n",
    "            not_up_epoch = 0\n",
    "            print(\n",
    "                'Validation accuracy %f increased from previous epoch, '\n",
    "                'save best_model' % score)\n",
    "            torch.save(model.state_dict(),\n",
    "                       os.path.join(save_model_path, \"best_model.pt\"))\n",
    "        else:\n",
    "            not_up_epoch += 1\n",
    "            if not_up_epoch > 100:\n",
    "                print(\n",
    "                    f\"Corrcoef didn't up for %s batch, early stop!\"\n",
    "                    % not_up_epoch)\n",
    "                break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6b74d3a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(device, model, dev_data, args):\n",
    "    model.eval()\n",
    "    \n",
    "    dataset = DatasetExtractor(dev_data, args.max_seq_len, args.max_attrname_len, args.max_attrvalue_len,\n",
    "                                     args.pretrain_model_path)\n",
    "    dataloader = DataLoader(dataset, batch_size=args.batch_size, shuffle=False)\n",
    "    \n",
    "    all_ids, all_logits, all_labels= [],[],[]\n",
    "    \n",
    "    for i_batch, data in enumerate(tqdm(dataloader)):\n",
    "        model.zero_grad()\n",
    "        text_id, inputs, label = data\n",
    "        token_ids = inputs[\"input_ids\"].flatten(1).to(device)\n",
    "        attention_mask = inputs[\"attention_mask\"].flatten(1).to(\n",
    "            device)\n",
    "        token_type_ids = inputs[\"token_type_ids\"].flatten(1).to(\n",
    "            device)\n",
    "        label = label.to(device)\n",
    "\n",
    "        logit = model(\n",
    "            token_ids=token_ids,\n",
    "            attention_mask=attention_mask,\n",
    "            token_type_ids=token_type_ids\n",
    "        )\n",
    "#         loss = loss_fn(logit, label)\n",
    "        \n",
    "        all_ids.append(text_id.detach().cpu().numpy())\n",
    "        all_logits.append(logit.detach().cpu().numpy())\n",
    "        all_labels.append(label.detach().cpu().numpy())\n",
    "    all_ids = np.concatenate(all_ids,)\n",
    "    all_logits = np.concatenate(all_logits,)\n",
    "    all_labels = np.concatenate(all_labels,)\n",
    "\n",
    "    auc, acc = cal_acc_score(all_ids, all_logits, all_labels)\n",
    "    return (auc, acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d6dc7ec3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading dataset...\n",
      "length of kb: 38054\n",
      "length of data: 19976\n",
      "length of sample: 208068\n",
      "length of data: 4757\n",
      "length of sample: 48781\n",
      "Creating model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at ../../pretrain_model/roberta-retrained/ were not used when initializing BertModel: ['cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.bias']\n",
      "- This IS expected if you are initializing BertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertModel were not initialized from the model checkpoint at ../../pretrain_model/roberta-retrained/ and are newly initialized: ['bert.pooler.dense.weight', 'bert.pooler.dense.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model created!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0/5: 100%|██████████| 6503/6503 [16:00<00:00,  6.77batches/s, Epoch=0, Loss=0.00422]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train step 1 auc=0.928930, acc=0.796456\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1525/1525 [01:14<00:00, 20.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid step 1 auc=0.951322, acc=0.863569\n",
      "Validation accuracy 0.863569 increased from previous epoch, save best_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1/5: 100%|██████████| 6503/6503 [15:52<00:00,  6.83batches/s, Epoch=1, Loss=0.00271]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train step 2 auc=0.962331, acc=0.880757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1525/1525 [01:11<00:00, 21.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid step 2 auc=0.953039, acc=0.865461\n",
      "Validation accuracy 0.865461 increased from previous epoch, save best_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2/5: 100%|██████████| 6503/6503 [15:45<00:00,  6.88batches/s, Epoch=2, Loss=0.00224]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train step 3 auc=0.970359, acc=0.899579\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1525/1525 [01:11<00:00, 21.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid step 3 auc=0.955559, acc=0.874291\n",
      "Validation accuracy 0.874291 increased from previous epoch, save best_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "3/5: 100%|██████████| 6503/6503 [15:46<00:00,  6.87batches/s, Epoch=3, Loss=0.00189]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train step 4 auc=0.976393, acc=0.914848\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1525/1525 [01:12<00:00, 20.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid step 4 auc=0.955889, acc=0.875342\n",
      "Validation accuracy 0.875342 increased from previous epoch, save best_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "4/5: 100%|██████████| 6503/6503 [15:52<00:00,  6.83batches/s, Epoch=4, Loss=0.00163]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train step 5 auc=0.981118, acc=0.928114\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1525/1525 [01:13<00:00, 20.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "valid step 5 auc=0.954641, acc=0.876603\n",
      "save final model for test...\n",
      "Validation accuracy 0.876603 increased from previous epoch, save best_model\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '0'\n",
    "parser = argparse.ArgumentParser()\n",
    "\n",
    "parser.add_argument('--mode', type=str, default='train')\n",
    "\n",
    "parser.add_argument('--train_file', type=str, default='../data/extractor_train.json')\n",
    "parser.add_argument('--dev_file', type=str, default='../data/extractor_valid.json')\n",
    "parser.add_argument('--kb_file', type=str, default='../../data/kg.json')\n",
    "parser.add_argument('--pretrain_model_path', type=str, default='../../pretrain_model/roberta-retrained/')\n",
    "parser.add_argument('--save_model_path', type=str, default='../model/intent_retrain_1')\n",
    "\n",
    "parser.add_argument('--gpu', type=int, default=0)\n",
    "parser.add_argument('--max_seq_len', type=int, default=64)\n",
    "parser.add_argument('--max_attrname_len', type=int, default=20)\n",
    "parser.add_argument('--max_attrvalue_len', type=int, default=40)\n",
    "parser.add_argument('--batch_size', type=int, default=32)\n",
    "parser.add_argument('--learning_rate', type=float, default=1e-5)\n",
    "parser.add_argument('--epochs', type=int, default=5)\n",
    "parser.add_argument('--validate_every', type=int, default=1)\n",
    "parser.add_argument('--patience', type=int, default=100)\n",
    "parser.add_argument('--fp16', type=bool, default=False)\n",
    "parser.add_argument(\"--fp16_opt_level\", type=str, default=\"O1\",\n",
    "                    help=\"For fp16: Apex AMP optimization level selected in ['O0', 'O1', 'O2', and 'O3'].\"\n",
    "                         \"See details at https://nvidia.github.io/apex/amp.html\", )\n",
    "\n",
    "sys.argv = ['']\n",
    "args = parser.parse_args()\n",
    "\n",
    "seed_everything(1)\n",
    "\n",
    "if args.mode == \"train\":\n",
    "    train(args)\n",
    "elif args.mode == \"dev\":\n",
    "    pass\n",
    "else:\n",
    "    pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "290d0818",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model created!\n",
    "# 0/5: 100%|██████████| 6503/6503 [16:58<00:00,  6.39batches/s, Epoch=0, Loss=0.00415]\n",
    "# train step 1 auc=0.930379, acc=0.800811\n",
    "# 100%|██████████| 1525/1525 [01:13<00:00, 20.86it/s]\n",
    "# valid step 1 auc=0.951145, acc=0.863569\n",
    "# Validation accuracy 0.863569 increased from previous epoch, save best_model\n",
    "\n",
    "# train step 1 auc=0.930570, acc=0.801412\n",
    "# 100%|██████████| 1525/1525 [01:18<00:00, 19.42it/s]\n",
    "# valid step 1 auc=0.950047, acc=0.860837\n",
    "# Validation accuracy 0.860837 increased from previous epoch, save best_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "961d9b6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train step 2 auc=0.964714, acc=0.888817\n",
    "# valid step 2 auc=0.954600, acc=0.874711"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e53bae65",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0/5: 100%|██████████| 6503/6503 [15:43<00:00,  6.89batches/s, Epoch=0, Loss=0.00412]\n",
    "# train step 1 auc=0.929693, acc=0.800010\n",
    "# 100%|██████████| 1525/1525 [01:10<00:00, 21.63it/s]\n",
    "# valid step 1 auc=0.949735, acc=0.857053\n",
    "# Validation accuracy 0.857053 increased from previous epoch, save best_model\n",
    "# 1/5: 100%|██████████| 6503/6503 [15:47<00:00,  6.86batches/s, Epoch=1, Loss=0.00269]\n",
    "# train step 2 auc=0.962865, acc=0.880807\n",
    "# 100%|██████████| 1525/1525 [01:09<00:00, 22.07it/s]\n",
    "# valid step 2 auc=0.953600, acc=0.866723\n",
    "# Validation accuracy 0.866723 increased from previous epoch, save best_model\n",
    "# 2/5: 100%|██████████| 6503/6503 [15:47<00:00,  6.86batches/s, Epoch=2, Loss=0.00225]\n",
    "# train step 3 auc=0.969862, acc=0.900180\n",
    "# 100%|██████████| 1525/1525 [01:09<00:00, 22.05it/s]\n",
    "# valid step 3 auc=0.954726, acc=0.868404\n",
    "# Validation accuracy 0.868404 increased from previous epoch, save best_model\n",
    "# 3/5: 100%|██████████| 6503/6503 [15:44<00:00,  6.88batches/s, Epoch=3, Loss=0.00191]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
